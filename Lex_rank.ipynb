{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from gensim.models.wrappers.fasttext import FastText as ft\n",
    "from scipy.spatial import distance\n",
    "import pandas as pd\n",
    "import MeCab\n",
    "\n",
    "\n",
    "def word2id(bow, word_id):\n",
    "    for w in bow:\n",
    "        if w not in word_id:\n",
    "            word_id[w] = len(word_id)\n",
    "    return word_id\n",
    "\n",
    "def compute_tf(sentences, word_id):\n",
    "    tf = np.zeros([len(sentences), len(word_id)])\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        for w in sentences[i]:\n",
    "            tf[i][word_id[w]] += 1\n",
    "    return tf\n",
    "\n",
    "def compute_df(sentences, word_id):\n",
    "    df = np.zeros(len(word_id))\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        exist = {}\n",
    "        for w in sentences[i]:\n",
    "            if w not in exist:\n",
    "                df[word_id[w]] += 1\n",
    "                exist[w] = 1\n",
    "            else:\n",
    "                continue\n",
    "    return df\n",
    "\n",
    "def compute_idf(sentences, word_id):\n",
    "    idf = np.zeros(len(word_id))\n",
    "    df = compute_df(sentences, word_id)\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        idf[i] = np.log(len(sentences)/df[i]) + 1\n",
    "    return idf\n",
    "\n",
    "def compute_tfidf(sentences):\n",
    "    word_id = {}\n",
    "\n",
    "    for sent in sentences:\n",
    "        word_id = word2id(sent, word_id)\n",
    "\n",
    "    tf = compute_tf(sentences, word_id)\n",
    "    idf = compute_idf(sentences, word_id)\n",
    "\n",
    "    tf_idf = np.zeros([len(sentences), len(word_id)])\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        tf_idf[i] = tf[i] * idf\n",
    "\n",
    "    return tf_idf\n",
    "\n",
    "def compute_cosine(v1, v2):\n",
    "\n",
    "    return 1 - distance.cosine(v1, v2)\n",
    "\n",
    "def sent2vec(bow, model_w):\n",
    "\n",
    "    vector = np.zeros(100)\n",
    "    N = len(bow)\n",
    "\n",
    "    for b in bow:\n",
    "        try:\n",
    "            vector += model_w[b]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    vector = vector / float(N)\n",
    "\n",
    "    return vector\n",
    "\n",
    "def compute_word2vec(sentences):\n",
    "\n",
    "    model = ft.load_fasttext_format(\"../Downloads/model\")\n",
    "    \n",
    "    vector = np.zeros([len(sentences), 100])\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        vector[i] = sent2vec(sentences[i], model_w)\n",
    "\n",
    "    return vector\n",
    "\n",
    "def lexrank(sentences, N, threshold, vectorizer):\n",
    "\n",
    "    CosineMatrix = np.zeros([N, N])\n",
    "    degree = np.zeros(N)\n",
    "    L = np.zeros(N)\n",
    "\n",
    "    if vectorizer == \"tf-idf\":\n",
    "        vector = compute_tfidf(sentences)\n",
    "    elif vectorizer == \"word2vec\":\n",
    "        vector = compute_word2vec(sentences)\n",
    "\n",
    "    # Computing Adjacency Matrix                                                                                                                                         \n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            CosineMatrix[i,j] = compute_cosine(vector[i], vector[j])\n",
    "            if CosineMatrix[i,j] > threshold:\n",
    "                CosineMatrix[i,j] = 1\n",
    "                degree[i] += 1\n",
    "            else:\n",
    "                CosineMatrix[i,j] = 0\n",
    "\n",
    "    # Computing LexRank Score                                                                                                                                            \n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            CosineMatrix[i,j] = CosineMatrix[i,j] / degree[i]\n",
    "\n",
    "    L = PowerMethod(CosineMatrix, N, err_tol=10e-6)\n",
    "\n",
    "    return L\n",
    "\n",
    "def PowerMethod(CosineMatrix, N, err_tol):\n",
    "\n",
    "    p_old = np.array([1.0/N]*N)\n",
    "    err = 1\n",
    "\n",
    "    while err > err_tol:\n",
    "        err = 1\n",
    "        p = np.dot(CosineMatrix.T, p_old)\n",
    "        err = np.linalg.norm(p - p_old)\n",
    "        p_old = p\n",
    "\n",
    "    return p\n",
    "\n",
    "def extractKeyword(text):\n",
    "    tagger = MeCab.Tagger()\n",
    "    tagger.parse('')\n",
    "    node = tagger.parseToNode(text).next\n",
    "    keywords = []\n",
    "    while node:\n",
    "        if node.feature.split(\",\")[0] == \"名詞\":\n",
    "            keywords.append(node.surface)\n",
    "        node = node.next\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.]\n"
     ]
    }
   ],
   "source": [
    "b = Hoge()\n",
    "b.hoge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Lexrank():\n",
    "    def __init__(self, sentences):\n",
    "    \n",
    "        self.sent = sentences\n",
    "    \n",
    "    def __tf_idf_(self):\n",
    "        self.tmp = {}\n",
    "        \n",
    "        for i in self.sent:\n",
    "            tmp = self.__word2id_()\n",
    "        \n",
    "    def compute_tfidf(sentences):\n",
    "    word_id = {}\n",
    "\n",
    "    for sent in sentences:\n",
    "        word_id = word2id(sent, word_id)\n",
    "\n",
    "    tf = compute_tf(sentences, word_id)\n",
    "    idf = compute_idf(sentences, word_id)\n",
    "\n",
    "    tf_idf = np.zeros([len(sentences), len(word_id)])\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        tf_idf[i] = tf[i] * idf\n",
    "\n",
    "    return tf_idf\n",
    "    \n",
    "    def fit(self,threshold,N=0,vectorizer='tf-idf'):\n",
    "        if N == 0:\n",
    "            self.N = len(self.sent)\n",
    "        \n",
    "        self.cos_matrix = np.zeros((self.N,self.N))\n",
    "        self.__degree_ = np.zeros(self.N)\n",
    "        self.score = np.zeros(self.N)\n",
    "        \n",
    "        if vectorizer == 'tf-idf':\n",
    "            self.vector = self.__tf_idf_()\n",
    "        else:\n",
    "            sys.exit('Vectorizer Error Occured!')\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "    CosineMatrix = np.zeros([N, N])\n",
    "    degree = np.zeros(N)\n",
    "    L = np.zeros(N)\n",
    "\n",
    "    if vectorizer == \"tf-idf\":\n",
    "        vector = compute_tfidf(sentences)\n",
    "    elif vectorizer == \"word2vec\":\n",
    "        vector = compute_word2vec(sentences)\n",
    "\n",
    "    # Computing Adjacency Matrix                                                                                                                                         \n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            CosineMatrix[i,j] = compute_cosine(vector[i], vector[j])\n",
    "            if CosineMatrix[i,j] > threshold:\n",
    "                CosineMatrix[i,j] = 1\n",
    "                degree[i] += 1\n",
    "            else:\n",
    "                CosineMatrix[i,j] = 0\n",
    "\n",
    "    # Computing LexRank Score                                                                                                                                            \n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            CosineMatrix[i,j] = CosineMatrix[i,j] / degree[i]\n",
    "\n",
    "    L = PowerMethod(CosineMatrix, N, err_tol=10e-6)\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kuchikomi = pd.read_csv('./test.csv',encoding=\"SHIFT-JIS\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = kuchikomi[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "インターンはたくさん参加しよう。 0.0833533165365\n",
      "ケース面接の本は、何冊か読めばインターンまではとりあえず行けると思います。 0.0714011814499\n",
      "インターン参加者の中には、なぜ自分が通ったのか疑問を感じる人も多く、もしかしたら面接官によってばらつきがあるのかもしれない。 0.0595734377148\n",
      "インターン参加者の質は非常に高く、他の外資金融、外資コンサルのインターン参加者ばかりで、他のインターンを通じて知り合いだったものも多いようだ。 0.0595511320714\n",
      "インターンの選考で重視されているのは、頭の良さと人柄だと思います。 0.0595125947791\n"
     ]
    }
   ],
   "source": [
    "stop = ['']\n",
    "sentences = [extractKeyword(tmp[x],stop) for x in range(len(tmp))]\n",
    "hoge = lexrank(sentences,21,0.10,'tf-idf')\n",
    "for k in range(5):\n",
    "    print(tmp[np.argsort(hoge)[::-1][k]], np.sort(hoge)[::-1][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ケース問題の対策にあたっては、「東大生の書いたフェルミ推定ノート」などケーススタディの参考書を一読することを勧めます。 0.047619047619\n",
      "あまりにもガツガツ行き過ぎて周りから非難されるような人は、たとえ優秀であったとしてもBCGは厳しいかもしれない。 0.047619047619\n",
      "１次面接でも２次面接でもケース問題をやり、いかにそこで論理的思考力があるかをアピールできるかにかかっています。 0.047619047619\n",
      "3日間という短い期間であるものの、参加している学生の頭脳レベルがとても高く、学生界で最高峰の議論、グループワークを経験できるという意味で非常に刺激になると思います。 0.047619047619\n"
     ]
    }
   ],
   "source": [
    "stop = ['インターン', 'ビーシージー']\n",
    "sentences = [extractKeyword(tmp[x],stop) for x in range(len(tmp))]\n",
    "n = 300\n",
    "\n",
    "k = np.random.rand(n)\n",
    "score = np.zeros(n)\n",
    "for i in range(n):\n",
    "    hoge = lexrank(sentences,21,k[i],'tf-idf')\n",
    "    score[i] = (np.sort(hoge)[::-1][0] - np.sort(hoge)[::-1][3]) / (np.sort(hoge)[::-1][0] - np.sort(hoge)[::-1][2])\n",
    "hoge = lexrank(sentences,21,k[score.argmax()],'tf-idf')\n",
    "for j in range(4):\n",
    "    print(tmp[np.argsort(hoge)[::-1][j]], np.sort(hoge)[::-1][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "インターン参加者の中には、なぜ自分が通ったのか疑問を感じる人も多く、もしかしたら面接官によってばらつきがあるのかもしれない。 0.0756966416305\n",
      "インターンに行く前はそういう風には考えられないとは思うが、インターンに受かったならば、BCGが出版している本をせめて一冊は読んでから、インターンに臨んでほしい。 0.0677295576006\n",
      "インターンで得た仲間とは今でも選考情報について交換することがあり、将来的にも大きな財産になると思います。 0.0677288496378\n",
      "3日間という短い期間であるものの、参加している学生の頭脳レベルがとても高く、学生界で最高峰の議論、グループワークを経験できるという意味で非常に刺激になると思います。 0.0637437746891\n"
     ]
    }
   ],
   "source": [
    "hoge = lexrank(sentences,21,0.01,'tf-idf')\n",
    "for j in range(4):\n",
    "    print(tmp[np.argsort(hoge)[::-1][j]], np.sort(hoge)[::-1][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "インターン参加者の中には、なぜ自分が通ったのか疑問を感じる人も多く、もしかしたら面接官によってばらつきがあるのかもしれない。 0.0871781833651\n",
      "インターンの選考で重視されているのは、頭の良さと人柄だと思います。 0.0769235922643\n",
      "ケース面接の本は、何冊か読めばインターンまではとりあえず行けると思います。 0.0666675850699\n",
      "インターンはたくさん参加しよう。 0.0615401832788\n"
     ]
    }
   ],
   "source": [
    "hoge = lexrank(sentences,21,0.02,'tf-idf')\n",
    "for j in range(4):\n",
    "    print(tmp[np.argsort(hoge)[::-1][j]], np.sort(hoge)[::-1][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "インターン参加者の中には、なぜ自分が通ったのか疑問を感じる人も多く、もしかしたら面接官によってばらつきがあるのかもしれない。 0.108529217486\n",
      "BCGの選考だからと身構えることなく、能力については基礎的な論理的思考力と少しの慣れがあれば十分だと思います。 0.0697645565719\n",
      "１次面接でも２次面接でもケース問題をやり、いかにそこで論理的思考力があるかをアピールできるかにかかっています。 0.0697585130913\n",
      "エントリーシートに書いた内容などは面接では一切触れられないので、志望動機や論理的思考以外の自己PRはあまり見られていないかもしれません。 0.0620086611189\n"
     ]
    }
   ],
   "source": [
    "for j in range(4):\n",
    "    print(tmp[np.argsort(hoge)[::-1][j]], np.sort(hoge)[::-1][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extractKeyword(text,stopword=['']):\n",
    "    tagger = MeCab.Tagger()\n",
    "    tagger.parse('')\n",
    "    node = tagger.parseToNode(text).next\n",
    "    keywords = []\n",
    "    hinshi = ['名詞','形容詞','動詞']\n",
    "    hinshi2 = ['接尾','非自立']\n",
    "    while node:\n",
    "        tmp = node.feature.split(',')\n",
    "        if tmp[0] in hinshi and tmp[1] not in hinshi2 and tmp[-1] not in stopword:\n",
    "            if tmp[6] == '*' and node.surface not in keywords:\n",
    "                keywords.append(node.surface)\n",
    "                #print(keywords[-1], node.feature)\n",
    "            elif tmp[6] not in keywords:\n",
    "                keywords.append(tmp[6])\n",
    "                #print(keywords[-1], node.feature)\n",
    "            else:\n",
    "                pass\n",
    "        node = node.next\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'「頭のよさ」とは具体的には、いわゆる論理的思考力があることと、クリエイティビティのような頭の柔らかさがあることが求められていると思います。'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "頭 名詞,一般,*,*,*,*,頭,アタマ,アタマ\n",
      "よい 形容詞,自立,*,*,形容詞・アウオ段,ガル接続,よい,ヨ,ヨ\n",
      "具体的 名詞,固有名詞,一般,*,*,*,具体的,グタイテキ,グタイテキ\n",
      "論理的思考 名詞,固有名詞,一般,*,*,*,論理的思考,ロンリテキシコウ,ロンリテキシコー\n",
      "ある 動詞,自立,*,*,五段・ラ行,基本形,ある,アル,アル\n",
      "クリエイティビティ 名詞,一般,*,*,*,*,*\n",
      "柔らかい 形容詞,自立,*,*,形容詞・アウオ段,ガル接続,柔らかい,ヤワラカ,ヤワラカ\n",
      "求める 動詞,自立,*,*,一段,未然形,求める,モトメ,モトメ\n",
      "思う 動詞,自立,*,*,五段・ワ行促音便,連用形,思う,オモイ,オモイ\n"
     ]
    }
   ],
   "source": [
    "x = extractKeyword(tmp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「 記号,括弧開,*,*,*,*,「,「,「\n",
      "頭 名詞,一般,*,*,*,*,頭,アタマ,アタマ\n",
      "の 助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "よ 形容詞,自立,*,*,形容詞・アウオ段,ガル接続,よい,ヨ,ヨ\n",
      "さ 名詞,接尾,特殊,*,*,*,さ,サ,サ\n",
      "」 記号,括弧閉,*,*,*,*,」,」,」\n",
      "と 助詞,格助詞,引用,*,*,*,と,ト,ト\n",
      "は 助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "具体的 名詞,固有名詞,一般,*,*,*,具体的,グタイテキ,グタイテキ\n",
      "に 助詞,格助詞,一般,*,*,*,に,ニ,ニ\n",
      "は 助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "、 記号,読点,*,*,*,*,、,、,、\n",
      "いわゆる 連体詞,*,*,*,*,*,いわゆる,イワユル,イワユル\n",
      "論理的思考 名詞,固有名詞,一般,*,*,*,論理的思考,ロンリテキシコウ,ロンリテキシコー\n",
      "力 名詞,接尾,一般,*,*,*,力,リョク,リョク\n",
      "が 助詞,格助詞,一般,*,*,*,が,ガ,ガ\n",
      "ある 動詞,自立,*,*,五段・ラ行,基本形,ある,アル,アル\n",
      "こと 名詞,非自立,一般,*,*,*,こと,コト,コト\n",
      "と 助詞,並立助詞,*,*,*,*,と,ト,ト\n",
      "、 記号,読点,*,*,*,*,、,、,、\n",
      "クリエイティビティ 名詞,一般,*,*,*,*,*\n",
      "の 助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "よう 名詞,非自立,助動詞語幹,*,*,*,よう,ヨウ,ヨー\n",
      "な 助動詞,*,*,*,特殊・ダ,体言接続,だ,ナ,ナ\n",
      "頭 名詞,一般,*,*,*,*,頭,アタマ,アタマ\n",
      "の 助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "柔らか 形容詞,自立,*,*,形容詞・アウオ段,ガル接続,柔らかい,ヤワラカ,ヤワラカ\n",
      "さ 名詞,接尾,特殊,*,*,*,さ,サ,サ\n",
      "が 助詞,格助詞,一般,*,*,*,が,ガ,ガ\n",
      "ある 動詞,自立,*,*,五段・ラ行,基本形,ある,アル,アル\n",
      "こと 名詞,非自立,一般,*,*,*,こと,コト,コト\n",
      "が 助詞,格助詞,一般,*,*,*,が,ガ,ガ\n",
      "求め 動詞,自立,*,*,一段,未然形,求める,モトメ,モトメ\n",
      "られ 動詞,接尾,*,*,一段,連用形,られる,ラレ,ラレ\n",
      "て 助詞,接続助詞,*,*,*,*,て,テ,テ\n",
      "いる 動詞,非自立,*,*,一段,基本形,いる,イル,イル\n",
      "と 助詞,格助詞,引用,*,*,*,と,ト,ト\n",
      "思い 動詞,自立,*,*,五段・ワ行促音便,連用形,思う,オモイ,オモイ\n",
      "ます 助動詞,*,*,*,特殊・マス,基本形,ます,マス,マス\n",
      "。 記号,句点,*,*,*,*,。,。,。\n",
      " BOS/EOS,*,*,*,*,*,*,*,*\n",
      "['「', '頭', 'の', 'よ', 'さ', '」', 'と', 'は', '具体的', 'に', 'は', '、', 'いわゆる', '論理的思考', '力', 'が', 'ある', 'こと', 'と', '、', 'クリエイティビティ', 'の', 'よう', 'な', '頭', 'の', '柔らか', 'さ', 'が', 'ある', 'こと', 'が', '求め', 'られ', 'て', 'いる', 'と', '思い', 'ます', '。', '']\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "\n",
    "tagger = MeCab.Tagger('-Ochasen')\n",
    "tagger.parse('')\n",
    "node = tagger.parseToNode(tmp[0]).next\n",
    "key = []\n",
    "while node:\n",
    "    print(node.surface,node.feature)\n",
    "    key.append(node.surface)\n",
    "    node = node.next\n",
    "print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
