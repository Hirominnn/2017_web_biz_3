{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from gensim.models.wrappers.fasttext import FastText as ft\n",
    "from scipy.spatial import distance\n",
    "import pandas as pd\n",
    "import MeCab\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "def word2id(bow, word_id):\n",
    "    for w in bow:\n",
    "        if w not in word_id:\n",
    "            word_id[w] = len(word_id)\n",
    "    return word_id\n",
    "\n",
    "def compute_tf(sentences, word_id):\n",
    "    tf = np.zeros([len(sentences), len(word_id)])\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        for w in sentences[i]:\n",
    "            tf[i][word_id[w]] += 1\n",
    "    return tf\n",
    "\n",
    "def compute_df(sentences, word_id):\n",
    "    df = np.zeros(len(word_id))\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        exist = {}\n",
    "        for w in sentences[i]:\n",
    "            if w not in exist:\n",
    "                df[word_id[w]] += 1\n",
    "                exist[w] = 1\n",
    "            else:\n",
    "                continue\n",
    "    return df\n",
    "\n",
    "def compute_idf(sentences, word_id):\n",
    "    idf = np.zeros(len(word_id))\n",
    "    df = compute_df(sentences, word_id)\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        idf[i] = np.log(len(sentences)/df[i]) + 1\n",
    "    return idf\n",
    "\n",
    "def compute_tfidf(sentences):\n",
    "    '''\n",
    "    INPUT\n",
    "        sentences: list型。文章のリストで、単語で分かち書きされている。\n",
    "            (例) [[w11,w12,w13],[w21,w22],[w31,w32,w33,w34]]\n",
    "    OUTPUT\n",
    "        tf_idf: np.array型。文章をtf-idf法によってベクトル化したもの。\n",
    "    '''\n",
    "    word_id = {}\n",
    "\n",
    "    for sent in sentences:\n",
    "        word_id = word2id(sent, word_id)\n",
    "\n",
    "    tf = compute_tf(sentences, word_id)\n",
    "    idf = compute_idf(sentences, word_id)\n",
    "\n",
    "    tf_idf = np.zeros([len(sentences), len(word_id)])\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        tf_idf[i] = tf[i] * idf\n",
    "\n",
    "    return tf_idf\n",
    "\n",
    "def compute_cosine(v1, v2):\n",
    "\n",
    "    return 1 - distance.cosine(v1, v2)\n",
    "\n",
    "def sent2vec(bow, model_w):\n",
    "\n",
    "    vector = np.zeros(100)\n",
    "    N = len(bow)\n",
    "\n",
    "    for b in bow:\n",
    "        try:\n",
    "            vector += model_w[b]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    vector = vector / float(N)\n",
    "\n",
    "    return vector\n",
    "\n",
    "def compute_word2vec(sentences):\n",
    "\n",
    "    model = ft.load_fasttext_format(\"../Downloads/model\")\n",
    "    \n",
    "    vector = np.zeros([len(sentences), 100])\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        vector[i] = sent2vec(sentences[i], model_w)\n",
    "\n",
    "    return vector\n",
    "\n",
    "def lexrank(sentences, N, threshold, vectorizer):\n",
    "\n",
    "    CosineMatrix = np.zeros([N, N])\n",
    "    degree = np.zeros(N)\n",
    "    L = np.zeros(N)\n",
    "\n",
    "    if vectorizer == \"tf-idf\":\n",
    "        vector = compute_tfidf(sentences)\n",
    "    elif vectorizer == \"word2vec\":\n",
    "        vector = compute_word2vec(sentences)\n",
    "\n",
    "    # Computing Adjacency Matrix                                                                                                                                         \n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            CosineMatrix[i,j] = compute_cosine(vector[i], vector[j])\n",
    "            if CosineMatrix[i,j] > threshold:\n",
    "                CosineMatrix[i,j] = 1\n",
    "                degree[i] += 1\n",
    "            else:\n",
    "                CosineMatrix[i,j] = 0\n",
    "\n",
    "    # Computing LexRank Score                                                                                                                                            \n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            CosineMatrix[i,j] = CosineMatrix[i,j] / degree[i]\n",
    "\n",
    "    L = PowerMethod(CosineMatrix, N, err_tol=10e-6)\n",
    "\n",
    "    return L\n",
    "\n",
    "def PowerMethod(CosineMatrix, N, err_tol):\n",
    "\n",
    "    p_old = np.array([1.0/N]*N)\n",
    "    err = 1\n",
    "\n",
    "    while err > err_tol:\n",
    "        err = 1\n",
    "        p = np.dot(CosineMatrix.T, p_old)\n",
    "        err = np.linalg.norm(p - p_old)\n",
    "        p_old = p\n",
    "\n",
    "    return p\n",
    "\n",
    "def extractKeyword(text):\n",
    "    tagger = MeCab.Tagger()\n",
    "    tagger.parse('')\n",
    "    node = tagger.parseToNode(text).next\n",
    "    keywords = []\n",
    "    while node:\n",
    "        if node.feature.split(\",\")[0] == \"名詞\":\n",
    "            keywords.append(node.surface)\n",
    "        node = node.next\n",
    "    return keywords\n",
    "\n",
    "def extractKeyword(text,stopword=['']):\n",
    "    tagger = MeCab.Tagger()\n",
    "    tagger.parse('')\n",
    "    node = tagger.parseToNode(text).next\n",
    "    keywords = []\n",
    "    hinshi = ['名詞','形容詞','副詞']\n",
    "    hinshi2 = ['接尾','非自立']\n",
    "    while node:\n",
    "        tmp = node.feature.split(',')\n",
    "        if tmp[0] in hinshi and tmp[1] not in hinshi2 and tmp[-1] not in stopword:\n",
    "            if tmp[6] == '*' and node.surface not in keywords:\n",
    "                keywords.append(node.surface)\n",
    "                #print(keywords[-1], node.feature)\n",
    "            elif tmp[6] not in keywords:\n",
    "                keywords.append(tmp[6])\n",
    "                #print(keywords[-1], node.feature)\n",
    "            else:\n",
    "                pass\n",
    "        node = node.next\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_lex(report_list, stop, alpha):\n",
    "    owakachi = [extractKeyword(report_list[x],stop) for x in range(len(report_list))]\n",
    "    for i in range(len(owakachi)):\n",
    "        while(1):\n",
    "            try:\n",
    "                if owakachi[i] == []:\n",
    "                    del owakachi[i]\n",
    "                else:\n",
    "                    break\n",
    "            except(IndexError):\n",
    "                break\n",
    "\n",
    "    result = lexrank(owakachi,21,alpha,'tf-idf')\n",
    "    \n",
    "    output = []\n",
    "    for k in range(3):\n",
    "        print(report_list[np.argsort(result)[::-1][k]], np.sort(result)[::-1][k])\n",
    "        output.append(report_list[np.argsort(result)[::-1][k]])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def open_json(path, translation):\n",
    "    report = open(path)\n",
    "    report = json.load(report)\n",
    "    report_list = []\n",
    "    tmp_report = report['selection_reports']\n",
    "    for i in range(len(tmp_report)):\n",
    "        tmp_report1 = list(tmp_report[i].values())\n",
    "        for j in range(len(tmp_report1)):\n",
    "            tmp_report1[j] = tmp_report1[j].translate(translation)\n",
    "            try:\n",
    "                if tmp_report1[j][-1] == '。':\n",
    "                    tmp_report2 = tmp_report1[j].split('。')\n",
    "                else:\n",
    "                    warnings.warn(str(tmp_report1[j]))\n",
    "                    tmp_report2 = tmp_report1[j].split('。')\n",
    "            except:\n",
    "                continue\n",
    "            for k in range(len(tmp_report2)):\n",
    "                if tmp_report2[k] != '':\n",
    "                    report_list.append(tmp_report2[k])\n",
    "    return report_list, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_company_name(raw):\n",
    "    tagger = MeCab.Tagger()\n",
    "    tagger.parse('')\n",
    "    node = tagger.parseToNode(raw['name']).next\n",
    "    return [node.feature.split(',')[6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagger = MeCab.Tagger()\n",
    "tagger.parse('')\n",
    "node = tagger.parseToNode(CA_raw['name']).next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "この会社に入社したいなら、何年目にどのポジションにつくために、まずはこの部署でこの仕事に取り組み、次にこのプランを実施したい、というような人生計画を持っていくべきです 0.0914128252404\n",
      "ただ、それよりも「一緒に働きたいかどうか」などの基本的な人としての部分が重視されていたと思う 0.076145862243\n",
      "学生時代どんなことをしてきたのか、そこで感じたことや学んだことがサイバーエージェント・web業界とどうリンクするのかは見られていた気がする 0.0761305834315\n"
     ]
    }
   ],
   "source": [
    "#サイバー\n",
    "report, raw = open_json('/Users/Shintaro1/Desktop/git/2017_web_biz_4/data/selection_reports/cyber_agent.json',\\\n",
    "                     str.maketrans('．', '。'))\n",
    "rank = do_lex(report, extract_company_name(raw), 0.1)\n",
    "output = {}\n",
    "output['name'] = raw['name']\n",
    "output['rank'] = rank\n",
    "f = open('/Users/Shintaro1/Desktop/lex_rank/cyber_agent.json','w')\n",
    "json.dump(output, f, ensure_ascii=False,indent=2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "将来に関しては、自分の思い描いている将来がDeNAで達成できるのかどうかを説明できるとなお良いかもしれない 0.0834482153652\n",
      "圧迫などは全くなく、本当に素の自分がどれだけ会社に合っているかというところを常に意識していたし、意識させられた 0.0696144813613\n",
      "本選考で見られていた点は●将来に対して自分がどのように有りたいか●内定を出したら、本当に来てくれるのかの2つだと思う 0.0695655846394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Shintaro1/.pyenv/versions/3.4.3/lib/python3.4/site-packages/ipykernel/__main__.py:13: UserWarning: 論理的思考ができるかどうかと、その人が元々持っている熱量がバランスよく見られていると感じた。ただ、どちらもないとダメと言うわけではなく、どちらか一方が図抜けている学生も歓迎されている印象を受けた。また、選考には非常に時間をかけている会社なので、下手にその場その場で対策しようとするよりは、素の自分を出せるかどうかの方が重要。逆に志望動機などは一切聞かれない。(DeNAを志望していない優秀な学生を心変わりさせるのが目的でもあるので当然といえば当然)\n"
     ]
    }
   ],
   "source": [
    "#DeNA\n",
    "report, raw = open_json('/Users/Shintaro1/Desktop/git/2017_web_biz_4/data/selection_reports/dena.json',\\\n",
    "                     str.maketrans('．！', '。。'))\n",
    "rank = do_lex(report, extract_company_name(raw), 0.1)\n",
    "output = {}\n",
    "output['name'] = raw['name']\n",
    "output['rank'] = rank\n",
    "f = open('/Users/Shintaro1/Desktop/lex_rank/dena.json','w')\n",
    "json.dump(output, f, ensure_ascii=False,indent=2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "広告業界に行きたい人だけが受かる会社ではないと言われつつも、広告に対する熱意を伝えることは大事だと思います 0.0714789212588\n",
      "広告についての知識はもちろん、その上で自分は何がやりたいのかを話せる必要があります 0.0714504837233\n",
      "広告業界というとすごく遠い世界のように感じてしまって、自信を持てない人もいると思いますが、自分のやってきたことを自信を持って話すことができれば可能性は開けてくると思います 0.0595783311426\n"
     ]
    }
   ],
   "source": [
    "#電通\n",
    "report, raw = open_json('/Users/Shintaro1/Desktop/git/2017_web_biz_4/data/selection_reports/dentsu.json',\\\n",
    "                     str.maketrans('．！', '。。'))\n",
    "rank = do_lex(report, extract_company_name(raw), 0.1)\n",
    "output = {}\n",
    "output['name'] = raw['name']\n",
    "output['rank'] = rank\n",
    "f = open('/Users/Shintaro1/Desktop/lex_rank/dentsu.json','w')\n",
    "json.dump(output, f, ensure_ascii=False,indent=2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Shintaro1/.pyenv/versions/3.4.3/lib/python3.4/site-packages/ipykernel/__main__.py:13: UserWarning: 。この仕事に興味を持っているか。誰が相手でもきちんとコミュニケーションをとることができるか（たとえ目上の方にでも臆せず話せるか）。自分の意見をはっきり主張できるか。この部門に合っているかどうか（適性）。ある程度の英語力（完璧とまではいかずとも、”頑張って話そうとしているか”）。ストレス耐性はあるか。どんな場面でも柔軟に対応できるか\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "オペレーションズの業務に支障のない英語力があるかどうか 0.0626596569701\n",
      "私が想定していた以上に”人柄”を重視した選考を行っているので、緊張する気持ちも分かりますが面接前に深呼吸をして、普段通りの自分で臨むと良いのではないでしょうか 0.0612244897957\n",
      "業務内容に関する理解が深まったら、なぜ自分自身がオペレーションズ部門の業務に対する適性があるのかを十分に考えると良いと思います 0.0501217775205\n"
     ]
    }
   ],
   "source": [
    "#ゴールドマン・サックス\n",
    "report, raw = open_json('/Users/Shintaro1/Desktop/git/2017_web_biz_4/data/selection_reports/goldman_sachs.json',\\\n",
    "                     str.maketrans('．！・', '。。。'))\n",
    "rank = do_lex(report, extract_company_name(raw), 0.1)\n",
    "output = {}\n",
    "output['name'] = raw['name']\n",
    "output['rank'] = rank\n",
    "f = open('/Users/Shintaro1/Desktop/lex_rank/goldman_sachs.json','w')\n",
    "json.dump(output, f, ensure_ascii=False,indent=2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今まで受けたIT企業の中でも、すごくプログラミング能力や実践的な思考能力、そして思考プロセスを重視した企業という印象を受けました 0.0877815235269\n",
      "もちろんある程度正確に問題を解くことは大切ですが、それ以上にどういう手順で問題を解いていくかのほうが重視されている印象がありました 0.068283188616\n",
      "選考に臨む前に向こうから提示される基礎知識のリストを総復習して、それをコーディングに落とせるようにしとくことが大切だと思います 0.0682655599236\n"
     ]
    }
   ],
   "source": [
    "#グーグル\n",
    "report, raw = open_json('/Users/Shintaro1/Desktop/git/2017_web_biz_4/data/selection_reports/google.json',\\\n",
    "                     str.maketrans('．！', '。。'))\n",
    "rank = do_lex(report, extract_company_name(raw), 0.05)\n",
    "output = {}\n",
    "output['name'] = raw['name']\n",
    "output['rank'] = rank\n",
    "f = open('/Users/Shintaro1/Desktop/lex_rank/google.json','w')\n",
    "json.dump(output, f, ensure_ascii=False,indent=2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Shintaro1/.pyenv/versions/3.4.3/lib/python3.4/site-packages/ipykernel/__main__.py:14: UserWarning: 【私の思うポイント】。チームで働けるか。一緒に働きたいか。能力（論理的か。。コンサルタントとしての適正【内定を頂いた際に伺ったフィードバック（どの点を見ていたのかの参考に。】GD。ゴールを意識して時間配分をして議論を進めていた。チームの意見を調整して成果を伸ばしていた面接。自分の言葉で発言しており、印象が良かった。人当たりが良く、信頼される人材である。志望動機が面白い\n",
      "/Users/Shintaro1/.pyenv/versions/3.4.3/lib/python3.4/site-packages/ipykernel/__main__.py:14: UserWarning: この会社は離職率が高く、また営業ということでかなり体力をようす部署であったので、男性と一緒に働けるかなどメンタルやフィジカル的なところが重視されていたと思います。また、営業は顧客とのコミュニケーションがメインの仕事なので、そのコミュニケーション能力もみられていたように思います。質問に的確に答えられるか、明るさや人当たりのよさもみられていたような気がします\n",
      "/Users/Shintaro1/.pyenv/versions/3.4.3/lib/python3.4/site-packages/ipykernel/__main__.py:14: UserWarning: 内定時のフィードバックによると、グループディスカッションでは自分の意見を伝えることができ、且つ周りの意見を聞きだし受け入れるバランス感覚が評価された。\n",
      "面接では、相手に伝えるために論理を効果的に用いたことが評価された。（例：ケース面接の前準備で、メリット。デメリットを数字で評価し、それを言葉で説明した。\n",
      "\n",
      "/Users/Shintaro1/.pyenv/versions/3.4.3/lib/python3.4/site-packages/ipykernel/__main__.py:14: UserWarning: 。論理的に物事を考えられるか。コミュニケーションを積極的にとれるか。他の人の意見を聞けるか。コンサルタント向きな考え方ができるか。将来の目標と今の自分のギャップを認識しているか。つまり結論は何か、を示せるか\n",
      "/Users/Shintaro1/.pyenv/versions/3.4.3/lib/python3.4/site-packages/ipykernel/__main__.py:14: UserWarning: ホームページを見ただけではイメージが描きにくいと思うので、説明会はエントリーシートのためにも面接のためにもとても役に立つと思う。エントリーシートの提出期限は２回に分かれているが、特にどちらが不利ということはないと思う（面接時に２回目の締め切りに出した人に会ったりしたから。。途中でTOEICの点数の提出を求められる。特にボーダーがあるとは思えないが、周りの受験者も高いスコアを持っているひとがたくさんいた\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自分の経験について、理路整然とはっきりと伝えられ、ビジョンと志望職種が一貫していることが求められるのではないかと思いました 0.0732499187527\n",
      "そこで、自分に一番合っている職種を選ぶのが非常に重要となるのではないかと思いました 0.0732300376365\n",
      "GDでは、積極性の有無と、資料を正しく読み込めるかどうか、自分なりにチームに貢献することができるかどうかが重視されていたと思われる 0.0612244897959\n"
     ]
    }
   ],
   "source": [
    "#日本IBM\n",
    "report, raw = open_json('/Users/Shintaro1/Desktop/git/2017_web_biz_4/data/selection_reports/japan_ibm.json',\\\n",
    "                     str.maketrans('．！・）', '。。。。'))\n",
    "rank = do_lex(report, extract_company_name(raw), 0.1)\n",
    "output = {}\n",
    "output['name'] = raw['name']\n",
    "output['rank'] = rank\n",
    "f = open('/Users/Shintaro1/Desktop/lex_rank/japan_ibm.json','w')\n",
    "json.dump(output, f, ensure_ascii=False,indent=2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'就職活動は理不尽なことも多く、落ち込むこともあるかと思います。ただ、周囲の意見、動向に流されることなく、自身の目指す目標に向かって頑張って欲しいと思います。友人、先輩など使えるリソースは全て使って、就職活動を楽しんでいただけたらと思います。道に迷ったときこそ、自分の信じるものに立ち返り、後悔のない選択をしていただけたら幸いです。みなさんの就職活動の成功を期待しています。。'"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#日本IBM\n",
    "report, raw = open_json('/Users/Shintaro1/Desktop/git/2017_web_biz_4/data/selection_reports/jp_morgan.json',\\\n",
    "                     str.maketrans('．！・）', '。。。。'))\n",
    "rank = do_lex(report, extract_company_name(raw), 0.1)\n",
    "output = {}\n",
    "output['name'] = raw['name']\n",
    "output['rank'] = rank\n",
    "f = open('/Users/Shintaro1/Desktop/lex_rank/japan_ibm.json','w')\n",
    "json.dump(output, f, ensure_ascii=False,indent=2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "この会社に入社したいなら、何年目にどのポジションにつくために、まずはこの部署でこの仕事に取り組み、次にこのプランを実施したい、というような人生計画を持っていくべきです 0.0914128252404\n",
      "ただ、それよりも「一緒に働きたいかどうか」などの基本的な人としての部分が重視されていたと思う 0.076145862243\n",
      "学生時代どんなことをしてきたのか、そこで感じたことや学んだことがサイバーエージェント・web業界とどうリンクするのかは見られていた気がする 0.0761305834315\n",
      "選考フローが人によって違うこともあるようだが、基本的に他社と比べると多くの選考を通過する必要が今年はあったようだ 0.0609398535163\n",
      "「サイバーエージェントっぽいかどうか」（それは見た目とかじゃなく精神性も含め）という軸で見られていると思う 0.0609184471618\n"
     ]
    }
   ],
   "source": [
    "stop = ['サイバーエージェント','選考']\n",
    "owakachi = [extractKeyword(report_list[x],stop) for x in range(len(report_list))]\n",
    "for i in range(len(owakachi)):\n",
    "    while(1):\n",
    "        try:\n",
    "            if owakachi[i] == []:\n",
    "                del owakachi[i]\n",
    "            else:\n",
    "                break\n",
    "        except(IndexError):\n",
    "            break\n",
    "\n",
    "CA = lexrank(owakachi,21,0.1,'tf-idf')\n",
    "CA_report = report_list\n",
    "for k in range(5):\n",
    "    print(report_list[np.argsort(CA)[::-1][k]], np.sort(CA)[::-1][k])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
